{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Universidad/TESIS/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Universidad/TESIS/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from time import gmtime, strftime\n",
    "from datetime import datetime, timedelta\n",
    "import unicodedata\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import requests\n",
    "import bs4 as bs\n",
    "from lxml import html\n",
    "from tqdm import tqdm\n",
    "from string import punctuation\n",
    "\n",
    "from pandas_datareader import data as pdr \n",
    "import yfinance as yf \n",
    "\n",
    "yf.pdr_override()\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "\"\"\"\n",
    "this is where different from version 1\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirección de la data:\n",
    "'./Scrape_data/Scrape_data/10_k/[cik]/grabbed_text/[cik]_[date]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de diccionario de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3145: DtypeWarning: Columns (15,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agilent Technologies Inc.</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American Airlines Group Inc.</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Advance Auto Parts Inc.</td>\n",
       "      <td>AAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AbbVie Inc.</td>\n",
       "      <td>ABBV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>Xylem Inc.</td>\n",
       "      <td>XYL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>Yum! Brands Inc.</td>\n",
       "      <td>YUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>Zimmer Biomet Holdings Inc.</td>\n",
       "      <td>ZBH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>Zions Bancorporation N.A.</td>\n",
       "      <td>ZION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>Zoetis Inc. Class A</td>\n",
       "      <td>ZTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>505 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name ticker\n",
       "0       Agilent Technologies Inc.      A\n",
       "1    American Airlines Group Inc.    AAL\n",
       "2         Advance Auto Parts Inc.    AAP\n",
       "3                      Apple Inc.   AAPL\n",
       "4                     AbbVie Inc.   ABBV\n",
       "..                            ...    ...\n",
       "500                    Xylem Inc.    XYL\n",
       "501              Yum! Brands Inc.    YUM\n",
       "502   Zimmer Biomet Holdings Inc.    ZBH\n",
       "503     Zions Bancorporation N.A.   ZION\n",
       "504           Zoetis Inc. Class A    ZTS\n",
       "\n",
       "[505 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ticker_library = pd.read_csv('tickers.csv')\n",
    "\n",
    "ticker_selected = pd.read_csv('SP500_component_stocks.csv',header = None)\n",
    "ticker_selected.columns = ['name','ticker']\n",
    "\n",
    "display(ticker_selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001090872</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000006201</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001158449</td>\n",
       "      <td>AAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000320193</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001551152</td>\n",
       "      <td>ABBV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cik ticker\n",
       "0  0001090872      A\n",
       "1  0000006201    AAL\n",
       "2  0001158449    AAP\n",
       "3  0000320193   AAPL\n",
       "4  0001551152   ABBV"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_cik_df = pd.DataFrame()\n",
    "\n",
    "ticker_list = ticker_selected.ticker.unique()\n",
    "\n",
    "cik_list = []\n",
    "\n",
    "for ticker in ticker_list:    \n",
    "    try:\n",
    "        cik_list.append(list(ticker_library[ticker_library.ticker == ticker].secfilings)[0][-10:])\n",
    "        \n",
    "    except:\n",
    "        cik_list.append('')\n",
    "\n",
    "ticker_cik_df['cik'] = cik_list\n",
    "ticker_cik_df['ticker'] = ticker_list\n",
    "\n",
    "ticker_cik_df = ticker_cik_df[ticker_cik_df['cik'] != '']\n",
    "\n",
    "ticker_cik_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a sample to analysis the overall program\n",
    "# sample1 = ticker_cik_df.iloc[0:50]\n",
    "# sample2 = ticker_cik_df.iloc[50:100]\n",
    "# sample3 = ticker_cik_df.iloc[100:150]\n",
    "# sample4 = ticker_cik_df.iloc[150:200]\n",
    "# sample5 = ticker_cik_df.iloc[200:250]\n",
    "# sample6 = ticker_cik_df.iloc[250:300]\n",
    "# sample7 = ticker_cik_df.iloc[300:350]\n",
    "# sample8 = ticker_cik_df.iloc[350:400]\n",
    "# sample9 = ticker_cik_df.iloc[400:450]\n",
    "# sample10 = ticker_cik_df.iloc[450:500]\n",
    "ticker_cik_sample = ['0000049826','0000816284']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {}\n",
    "\n",
    "for cik, ticker in zip(cik_list, ticker_list):\n",
    "    \n",
    "    all_data[ticker] = {}\n",
    "\n",
    "    all_data[ticker]['cik'] = cik\n",
    "    all_data[ticker]['10ks'] = {}\n",
    "    all_data[ticker]['10qs'] = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [

   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathname_10k = './Scrape_data/10_k/'\n",
    "pathname_10q = './Scrape_data/10_q/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    
    "\n",
    "def Get_Ex_Ret(ticker,fillingdate):\n",
    "    \n",
    "    start = datetime.strptime(fillingdate, '%Y-%m-%d')\n",
    "    \n",
    "    \"\"\"\n",
    "    change from 10 to 120 to ensure engough days\n",
    "    \"\"\"\n",
    "    end = start + timedelta(120)\n",
    "    \n",
    "    price_120_days = pdr.get_data_yahoo(ticker, start, end)\n",
    "    \n",
    "    \"\"\"\n",
    "    change from 5 days to 63 days---which is the length of three month\n",
    "    \"\"\"\n",
    "    ret_4d = price_120_days.iloc[62]['Adj Close'] / price_120_days.iloc[0]['Adj Close'] - 1\n",
    "    return 100 * ret_4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(string):\n",
    "    return re.sub(r\"[{}]+\".format(punctuation), \"\", string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(string):\n",
    "    return bool(re.match(r'^[a-z\\']+$', string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for ticker in all_data.keys():\n",
    "    \n",
    "    cik = all_data[ticker]['cik']\n",
    "    \n",
    "    try:\n",
    "        files_10k = os.listdir(pathname_10k + cik + '/grabbed_text/')\n",
    "    except:\n",
    "        break\n",
    "    \n",
    "    for file_10k in files_10k:\n",
    "        \n",
    "        release_10k = file_10k[-14:-4]\n",
    "        \n",
    "        \"\"\"\n",
    "        adding beta = 1, denoting the market return value at that time to calculate excess return.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            excess_return = Get_Ex_Ret(ticker, release_10k)\n",
    "            \"\"\"\n",
    "            adding a market return term and subtracting it(assuming beta = 1)\n",
    "            \"\"\"\n",
    "            market_return = Get_Ex_Ret('SPY', release_10k)\n",
    "            excess_return = excess_return - market_return\n",
    "            \n",
    "        except:\n",
    "            break\n",
    "    \n",
     "        with open(pathname_10k + cik + '/grabbed_text/' + file_10k, encoding = \"utf8\") as f:\n",
    "            string_temp = f.read().lower()\n",
    "        \n",
    "        filtered_words = [remove_punct(word) for word in string_temp.split() if word not in stop_words]\n",
    "        \n",
    "        filtered_words = list(filter(filter_words, filtered_words))\n",
    "        \n",
    "        filtered_words = [lemmatizer.lemmatize(word, pos = 'v') for word in filtered_words]\n",
    "        filtered_words = [lemmatizer.lemmatize(word, pos = 'n') for word in filtered_words]\n",
    "        \n",
    "        \n",
    "        if filtered_words != []:\n",
    "            all_data[ticker]['10ks'][release_10k] = {}\n",
    "            all_data[ticker]['10ks'][release_10k]['ex_return'] = excess_return\n",
    "            all_data[ticker]['10ks'][release_10k]['words'] = ' '.join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is where different from version 1, adding a beta term to calculate excess return.\n",
    "\"\"\"\n",
    "\n",
    "for ticker in all_data.keys():\n",
    "    \n",
    "    cik = all_data[ticker]['cik']\n",
    "    \n",
    "    try:\n",
    "        files_10q = os.listdir(pathname_10q + cik + '/grabbed_text/')\n",
    "    except:\n",
    "        break\n",
    "    \n",
    "    for file_10q in files_10q:\n",
    "        \n",
    "        release_10q = file_10q[-14:-4]       \n",
    "        \n",
    "        \n",
    "        try:\n",
    "            excess_return = Get_Ex_Ret(ticker, release_10q)\n",
    "            \"\"\"\n",
    "            adding a market return term and subtracting it(assuming beta = 1)\n",
    "            \"\"\"\n",
    "            market_return = Get_Ex_Ret('SPY', release_10k)\n",
    "            excess_return = excess_return - market_return\n",
    "        \n",
    "        except:\n",
    "            break\n",
    "    \n",
    "        with open(pathname_10q + cik + '/grabbed_text/' + file_10q, encoding = \"utf8\") as f:\n",
    "            string_temp = f.read().lower()\n",
    "        \n",
    "        filtered_words = [remove_punct(word) for word in string_temp.split() if word not in stop_words]\n",
    "        \n",
    "        filtered_words = list(filter(filter_words, filtered_words))\n",
    "        \n",
    "        filtered_words = [lemmatizer.lemmatize(word, pos = 'v') for word in filtered_words]\n",
    "        filtered_words = [lemmatizer.lemmatize(word, pos = 'n') for word in filtered_words]\n",
    "        \n",
    "        \n",
    "        if filtered_words != []:\n",
    "            all_data[ticker]['10qs'][release_10q] = {}\n",
    "            all_data[ticker]['10qs'][release_10q]['ex_return'] = excess_return\n",
    "            all_data[ticker]['10qs'][release_10q]['words'] = ' '.join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estructura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
   
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis de 10K con tfidf y bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is to compute the tf values for the words in 10ks and collect the overall word list for computing idf in the next step\n",
    "\"\"\"\n",
    "document_num_10k = 0\n",
    "\n",
    "word_list_10k = defaultdict(int)\n",
    "\n",
    "\n",
    "for ticker in all_data:\n",
    "    \n",
    "    for date in all_data[ticker]['10ks']:\n",
    "        \n",
    "        document_num_10k += 1\n",
    "        \n",
    "        all_data[ticker]['10ks'][date]['tf'] = Counter(all_data[ticker]['10ks'][date]['words'].split())\n",
    "        \n",
    "        for word in all_data[ticker]['10ks'][date]['tf']:\n",
    "            \n",
    "            word_list_10k[word] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_10k = {}\n",
    "\n",
    "for word in word_list_10k:\n",
    "    \n",
    "    idf_10k[word] = np.log(document_num_10k / (1 + word_list_10k[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "pair_list_10k = defaultdict(int)\n",
    "\n",
    "for ticker in all_data:\n",
    "    \n",
    "    for date in all_data[ticker]['10ks']:\n",
    "        \n",
    "        vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (2, 2))\n",
    "        \n",
    "        fitted = vectorizer.fit_transform([all_data[ticker]['10ks'][date]['words']])\n",
    "        df_temp = pd.DataFrame(index = vectorizer.get_feature_names(), data = np.squeeze(fitted.toarray()))\n",
    "        all_data[ticker]['10ks'][date]['tf_pair'] = df_temp.to_dict()[0]\n",
    "        for pair in all_data[ticker]['10ks'][date]['tf_pair']:\n",
    "            \n",
    "            pair_list_10k[pair] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_list_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_10k_pair = {}\n",
    "\n",
    "for pair in pair_list_10k:\n",
    "    \n",
    "    idf_10k_pair[pair] = np.log(document_num_10k / (1 + pair_list_10k[pair]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_list_10k = defaultdict(int)\n",
    "\n",
    "\n",
    "for ticker in all_data:\n",
    "    \n",
    "    for date in all_data[ticker]['10ks']:\n",
    "        \n",
    "        vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (3, 3))\n",
    "        \n",
    "        fitted = vectorizer.fit_transform([all_data[ticker]['10ks'][date]['words']])\n",
    "        \n",
    "        df_temp = pd.DataFrame(index = vectorizer.get_feature_names(), data = np.squeeze(fitted.toarray()))\n",
    "        \n",
    "        all_data[ticker]['10ks'][date]['tf_triple'] = df_temp.to_dict()[0]\n",
    "        \n",
    "        for triple in all_data[ticker]['10ks'][date]['tf_triple']:\n",
    "            \n",
    "            triple_list_10k[triple] += 1\n",
    "            \n",
    "idf_10k_triple = {}\n",
    "\n",
    "for triple in triple_list_10k:\n",
    "    \n",
    "    idf_10k_triple[triple] = np.log(document_num_10k / (1 + triple_list_10k[triple]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doing the same to 10qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Misma ejecución para 10 Q\n",
    "\n",
    "document_num_10q = 0\n",
    "\n",
    "word_list_10q = defaultdict(int)\n",
    "\n",
    "\n",
    "for ticker in all_data:\n",
    "    \n",
    "    for date in all_data[ticker]['10qs']:\n",
    "        \n",
    "        document_num_10q += 1\n",
    "        \n",
    "        all_data[ticker]['10qs'][date]['tf'] = Counter(all_data[ticker]['10qs'][date]['words'].split())\n",
    "        \n",
    "        for word in all_data[ticker]['10qs'][date]['tf']:\n",
    "            \n",
    "            word_list_10q[word] += 1\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_10q = {}\n",
    "\n",
    "for word in word_list_10q:\n",
    "    \n",
    "    idf_10q[word] = np.log(document_num_10q / (1 + word_list_10q[word]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_list_10q = defaultdict(int)\n",
    "\n",
    "for ticker in all_data:\n",
    "    \n",
    "    for date in all_data[ticker]['10qs']:\n",
    "        \n",
    "        vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (2, 2))\n",
    "        \n",
    "        fitted = vectorizer.fit_transform([all_data[ticker]['10qs'][date]['words']])\n",
    "        \n",
    "        df_temp = pd.DataFrame(index = vectorizer.get_feature_names(), data = np.squeeze(fitted.toarray()))\n",
    "        \n",
    "        all_data[ticker]['10qs'][date]['tf_pair'] = df_temp.to_dict()[0]\n",
    "        \n",
    "        for pair in all_data[ticker]['10qs'][date]['tf_pair']:\n",
    "            \n",
    "            pair_list_10q[pair] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_10q_pair = {}\n",
    "\n",
    "for pair in pair_list_10q:\n",
    "    \n",
    "    idf_10q_pair[pair] = np.log(document_num_10q / (1 + pair_list_10q[pair]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "triple_list_10q = defaultdict(int)\n",
    "\n",
    "\n",
    "for ticker in all_data:\n",
    "    \n",
    "    for date in all_data[ticker]['10qs']:\n",
    "        \n",
    "        vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (3, 3))\n",
    "        \n",
    "        fitted = vectorizer.fit_transform([all_data[ticker]['10qs'][date]['words']])\n",
    "        \n",
    "        df_temp = pd.DataFrame(index = vectorizer.get_feature_names(), data = np.squeeze(fitted.toarray()))\n",
    "        \n",
    "        all_data[ticker]['10qs'][date]['tf_triple'] = df_temp.to_dict()[0]\n",
    "        \n",
    "        for triple in all_data[ticker]['10qs'][date]['tf_triple']:\n",
    "            \n",
    "            triple_list_10q[triple] += 1\n",
    "            \n",
    "idf_10q_triple = {}\n",
    "\n",
    "for triple in triple_list_10q:\n",
    "    \n",
    "    idf_10q_triple[triple] = np.log(document_num_10q / (1 + triple_list_10q[triple]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estructura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_10q_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_10q_triple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the data for future use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in all_data:\n",
    "    \n",
    "    for date in all_data[ticker]['10qs']:\n",
    "        \n",
    "        del all_data[ticker]['10qs'][date]['words']\n",
    "    \n",
    "    for date in all_data[ticker]['10ks']:\n",
    "        \n",
    "        del all_data[ticker]['10ks'][date]['words']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_data.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(all_data))\n",
    "\n",
    "with open('idf_10k.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10k))\n",
    "\n",
    "with open('idf_10q.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10q))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('idf_10k_pair.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10k_pair))\n",
    "\n",
    "with open('idf_10q_pair.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10q_pair))\n",
    "    \n",
    "with open('idf_10k_triple.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10k_triple))\n",
    "\n",
    "with open('idf_10q_triple.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10q_triple))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
